{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d1054c",
   "metadata": {},
   "source": [
    "# CMP7005 – Programming for Data Analysis\n",
    "## PRAC1 – Air Quality Application (Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b185aca",
   "metadata": {},
   "source": [
    "## Task 1: Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0eab4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.2)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/sakhi/Downloads/chatbot/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Step 1: Define data directory (update the path below to your dataset folder)\n",
    "data_dir = 'PRSA_Data_20130301-20170228'  # Change this!\n",
    "\n",
    "# Step 2: Select four files (Urban, Suburban, Rural, Industrial)\n",
    "files = {\n",
    "    'Aotizhongxin (Urban)': 'PRSA_Data_Aotizhongxin_20130301-20170228.csv',\n",
    "    'Changping (Suburban)': 'PRSA_Data_Changping_20130301-20170228.csv',\n",
    "    'Huairou (Rural)': 'PRSA_Data_Huairou_20130301-20170228.csv',\n",
    "    'Nongzhanguan (Industrial)': 'PRSA_Data_Nongzhanguan_20130301-20170228.csv'\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Step 3: Load and label each dataset\n",
    "for site_name, filename in files.items():\n",
    "    path = os.path.join(data_dir, filename)\n",
    "    df = pd.read_csv(path)\n",
    "    df['site'] = site_name\n",
    "    dfs.append(df)\n",
    "\n",
    "# Step 4: Merge datasets\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Data merged successfully. Shape:\", combined_df.shape)\n",
    "\n",
    "# Step 5: Drop duplicates\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 6: Create datetime column\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df[['year', 'month', 'day', 'hour']])\n",
    "combined_df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Step 7: Drop rows with critical pollutant nulls\n",
    "critical_cols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "combined_df.dropna(subset=critical_cols, inplace=True)\n",
    "\n",
    "# Step 8: Fill remaining missing values (linear interpolation)\n",
    "combined_df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Step 9: Feature Engineering\n",
    "combined_df['season'] = combined_df['month'] % 12 // 3 + 1  # 1=Winter, 2=Spring, etc.\n",
    "\n",
    "# Step 10: Save cleaned dataset\n",
    "output_path = 'cleaned_air_quality.csv'\n",
    "combined_df.to_csv(output_path)\n",
    "print(f\"Cleaned dataset saved as: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df82471",
   "metadata": {},
   "source": [
    "## Task 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc14f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2a: Fundamental Data Understanding ---\n",
    "print(\"hape of the dataset:\", combined_df.shape)\n",
    "\n",
    "print(\"\\n Data Types:\")\n",
    "print(combined_df.dtypes)\n",
    "\n",
    "print(\"\\n Missing Values:\")\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "print(\"\\n Summary Statistics:\")\n",
    "print(combined_df.describe(include='all'))\n",
    "\n",
    "# --- Step 2b: Data Preprocessing ---\n",
    "print(\"\\n Dropping duplicates...\")\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\" Creating 'datetime' column...\")\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df[['year', 'month', 'day', 'hour']])\n",
    "combined_df.set_index('datetime', inplace=True)\n",
    "\n",
    "print(\" Dropping rows with missing critical pollutant values...\")\n",
    "combined_df.dropna(subset=['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3'], inplace=True)\n",
    "\n",
    "print(\" Interpolating remaining missing values...\")\n",
    "combined_df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "print(\" Adding 'season' as a feature...\")\n",
    "combined_df['month'] = combined_df['month'].astype(int)\n",
    "combined_df['season'] = combined_df['month'] % 12 // 3 + 1  # Winter=1, Spring=2, etc.\n",
    "\n",
    "# --- Step 2c: Statistics & Visualizations ---\n",
    "\n",
    "# Set up visual style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Histogram: PM2.5\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(combined_df['PM2.5'].dropna(), bins=50, kde=True)\n",
    "plt.title('Distribution of PM2.5')\n",
    "plt.xlabel('PM2.5 Concentration')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plot: PM2.5 vs Temperature\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(x='TEMP', y='PM2.5', data=combined_df)\n",
    "plt.title('PM2.5 vs Temperature')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "cols = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM']\n",
    "corr = combined_df[cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Air Quality and Weather Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2d1b0",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3: Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Select features and target\n",
    "features = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'WSPM']\n",
    "target = 'PM2.5'\n",
    "\n",
    "# Drop any remaining rows with nulls in selected columns\n",
    "ml_df = combined_df[features + [target]].dropna()\n",
    "\n",
    "# Split data\n",
    "X = ml_df[features]\n",
    "y = ml_df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(\"\\n Model Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
